## Kilas Pertama Visual SLAM

### Tujuan Studi

1. Memahami apa kerangka modul visual SLAM, dan apa tugas masing-masing modul.
2. Mengatur pemrograman environment, dan mempersiapkan diri untuk pengembangan dan percobaan.
3. Memahami bagaimana untuk mengkompilasi dan menjalankan program di bawah Linux. Jika ada masalah, bagaimana cara men-debugnya.
4. Menguasai dasar penggunaan cmake.

### Pengantar Kuliah

Kuliah ini merangkum struktur sistem visual SLAM sebagai garis besar pada bab-bab berikutnya. Bagian praktek memperkenalkan dasar-dasar environment setup dan program pembangunan. Kami akan menyelesaikan program "Hello SLAM" pada bagian akhir nanti.

### Menemui "Little Carrot"

Misalkan kita merakit robot bernama "Little Carrot", seperti ditunjukkan pada sketsa berikut:

![carrot](/resources/whatIsSLAM/carrot.jpg)

Meskipun terlihat mirip seperti robot Android, itu tidak ada hubungannya dengan sistem Android. Kami menempatkan laptop ke belalainya (sehingga kita dapat men-debug program setiap saat). Jadi, Apa yang mampu dilakukan robot?

Kami berharap Little Carrot memiliki kemampuan **bergerak secara otonom**. Meskipun ada "robot" yang ditempatkan statis pada desktop, ia mampu melakukan chatting dengan orang-orang dan bermain musik, komputer tablet saat ini dapat memberikan tugas-tugas yang sama. Sebagai robot, semoga Little Carrot dapat bergerak dengan bebas di ruangan. Tidak peduli ketika kita mengatakan halo, ia akan segera datang.

Pertama-tama, sebuah robot membutuhkan roda dan motor untuk bergerak, sehingga kita diinstal roda di bawah Little Carrot (gait kontrol untuk robot-robot humanoid sangat rumit, yang kita tidak akan mempertimbangkannya di sini). Sekarang dengan roda, robot dapat bergerak, tetapi tanpa sistem kontrol yang efektif, Little Carrot tidak tahu mana target manay yang melakukan tindakan, dan itu bisa melakukan apa-apa tapi berkeliaran membabi buta. Lebih buruk lagi, mungkin memukul dinding dan menyebabkan kerusakan. Untuk menghindari hal terjadi, kami memasang kamera di atas kepalanya, dengan intuisi bahwa robot **harus terlihat mirip manusia** - kita bisa melihatnya dari sketsa. Dengan mata, otak dan tungkai, manusia dapat berjalan bebas dan menjelajahi lingkungan apapun, jadi kami (mungkin naif) berpikir bahwa robot kami harus mampu mencapainya. Untuk membuat Little Carrot dapat menjelajahi ruang, dibutuhkan untuk mengetahui setidaknya dua hal:

1. Dimana saya? - penempatan
2. seperti apa lingkungan di sekitarnya? - pembangunan peta

"Penempatan" dan "pembangunan peta", dapat dilihat sebagai persepsi dua arah: ke dalam dan ke luar. Sebagai robot yang baik, _Little Carrot_ tidak perlu memahami **negara** (lokasinya) sendiri, tetapi juga lingkungan eksternalnya (peta). Tentu saja, ada banyak cara untuk mengatasi kedua masalah ini. Misalnya, kita bisa meletakkan rel pemandu di lantai ruangan, atau spidol stick seperti gambar yang berisi kode QR di dinding, atau memasang perangkat posisi radio di atas meja. Jika Anda berada di luar ruangan, Anda juga dapat memasang komponen penempatan (seperti yang ada pada ponsel atau mobil) di kepala _Little Carrot_. Dengan perangkat ini, dapatkah kita mengklaim bahwa masalah penentuan penempatan sudah terpecahkan? Mari kita kategorikan sensor ini (see \ autoref {fig: sensors}) menjadi dua kelas.

![sensor](/resources/whatIsSLAM/sensors.jpg)

Kelas pertama adalah sensor **non-intrusif** milik robot sendiri, seperti pengatur roda, kamera, pemindai laser, dll. Mereka tidak menganggap lingkungannya kooperatif. Kelas yang lain adalah sensor **intrusive** tergantung pada lingkungan yang dipersiapkan, seperti rel pemandu yang disebutkan di atas, stiker QR code, dll. Sensor _intrusive_ biasanya dapat menemukan robot secara langsung, menyelesaikan masalah penentuan posisi dengan cara yang sederhana dan efektif. Namun, karena membutuhkan perubahan lingkungan, ruang lingkup penggunaannya seringkali terbatas. Misalnya, bagaimana jika tidak ada sinyal GPS, atau rel yang sudah dibangun tidak bisa diletakkan? Apa yang harus kita lakukan pada kasus-kasus tersebut?

Kita dapat melihat bahwa sensor _intrusive_ menempatkan batasan tertentu pada lingkungan eksternal. Hanya saja jika kendala ini terpenuhi, sistem pelokalan yang berbasis pada mereka dapat berfungsi dengan baik. Jika kendala tidak bisa dipenuhi, penempatan tidak bisa dilakukan lagi. Karena itu, meski sensor jenis ini sederhana dan bisa diandalkan, mereka tidak memberikan solusi umum. Sebaliknya, sensor _non-intrusive_, seperti pemindai laser, kamera, pengatur roda, Inertial Measurement Units (IMUs), dll, hanya dapat mengamati jumlah fisik tidak langsung daripada lokasi langsung. Misalnya, alat pengukur roda mengukur sudut di mana roda berputar, IMU mengukur kecepatan sudut dan percepatan gerakan, kamera atau pemindai laser mengamati lingkungan eksternal dalam bentuk tertentu. Kita harus menerapkan algoritma untuk menyimpulkan penempatan dari pengamatan tidak langsung ini. Meskipun ini terdengar seperti taktik bundaran, manfaat yang lebih jelas adalah tidak memberikan tuntutan terhadap lingkungan, sehingga memungkinkan kerangka penentuan posisi ini diterapkan ke lingkungan yang tidak diketahui.

Melihat kembali definisi SLAM yang telah dibahas sebelumnya, kami menekankan **unknown environment** dalam masalah SLAM. Secara teori, kita seharusnya tidak menganggap lingkungan di mana ketika _Little Carrot_ akan digunakan (namun kenyataannya kita akan memiliki jarak yang kasar, seperti indoor atau outdoor), yang berarti kita tidak dapat mengasumsikan bahwa sensor eksternal seperti GPS dapat bekerja dengan baik. Oleh karena itu, penggunaan sensor portable _non-intrusive_ untuk mencapai SLAM adalah fokus utama kami. Secara khusus, ketika berbicara tentang visual SLAM, umumnya kita mengacu pada penggunaan kamera untuk memecahkan masalah pemetaan posisi dan pemetaan.

Visual SLAM adalah subjek utama buku ini, jadi kami sangat tertarik dengan apa yang bisa dilakukan oleh _Little Carrot_. Kamera yang digunakan di SLAM berbeda dengan kamera SLR yang biasa dilihat. Seringkali lebih sederhana dan tidak memakai lensa yang mahal. Ini membidik di lingkungan sekitar pada tingkatan-tingkatan tertentu, membentuk aliran video secara terus menerus. Kamera biasa bisa menangkap gambar 30 frame per detik, sementara pada kamera berkecepatan tinggi bidikannya lebih cepat. Kamera bisa dibagi menjadi tiga kategori: Monocular, Stereo dan RGB-D, seperti yang ditunjukkan oleh gambar berikut [kamera]. Secara intuitif, kamera monocular hanya memiliki satu kamera, dan dua kamera stereo. Prinsip kamera RGB-D lebih kompleks, selain mampu mengumpulkan gambar berwarna, juga bisa mengukur jarak _scene_ dari kamera untuk setiap _pixel_. Kamera RGB-D biasanya membawa banyak kamera, dan mungkin mengadopsi berbagai prinsip kerja yang berbeda. Dalam kuliah kelima, kami akan merincikan prinsip kerja mereka, dan pembaca hanya membutuhkan kesan intuitif untuk saat ini. Selain itu, ada juga tipe kamera khusus dan yang muncul yang bisa diaplikasikan ke SLAM, seperti kamera panorama [Pretto2011], dan event camera [Rueckauer2016]. Meski terkadang mereka terlihat dalam aplikasi SLAM, sejauh ini mereka belum mainstream digunakan. Dari paparan tersebut kita bisa simpulkan bahwa _Little Carrot_ nampaknya mengusung kamera stereo (akan sangat menakutkan jika monokuler, bayangkan saja...).

![kamera](/resources/whatIsSLAM/camera.jpg)

Sekarang, mari kita lihat pro kontra penggunaan berbagai jenis kamera untuk SLAM.

#### Kamera Monocular

Praktik penggunaan satu kamera untuk SLAM disebut _Monocular_ SLAM. Struktur sensor ini sangat sederhana, dan biayanya sangat rendah, oleh karena itu SLAM _monocular_ sangat menarik bagi peneliti. Anda pasti pernah melihat data output kamera _monocular_: foto. Ya, seperti foto, apa karakteristiknya?

Pada dasarnya foto adalah **proyeksi** dari sebuah adegan ke pesawat pencitraan kamera. **Ini mencerminkan dunia tiga dimensi dalam bentuk dua dimensi.** Jelas, ada satu dimensi yang hilang selama proses proyeksi ini, yang merupakan kedalaman (atau jarak) yang disebutkan. Dalam kasus _monocular_, kita tidak dapat memperoleh **jarak** antara objek di tempat kejadian dengan kamera dengan menggunakan satu gambar. Nantinya kita akan melihat bahwa jarak ini sebenarnya sangat penting bagi SLAM. Karena kita (manusia) telah melihat sejumlah besar gambar, kita membentuk jarak pandang alami untuk adegan yang banyak, dan ini dapat membantu kita menentukan jarak hubungan antara objek dalam gambar. Misalnya, kita dapat mengenali objek dalam gambar dan mengkorelasikannya dengan ukuran perkiraan yang diperoleh dari pengalaman sehari-hari. Contohnya adalah: objek dekat akan menutup objek yang jauh; matahari, bulan dan benda langit lainnya umumnya jauh; jika sebuah objek akan memiliki bayangan jika berada di bawah cahaya; dan seterusnya. Hal ini bisa membantu kita menentukan jarak objek, tetapi ada beberapa kasus yang dapat membingungkan kita, dan kita tidak bisa lagi menentukan jarak dan ukuran sebenarnya dari suatu objek. Gambar berikut [why-depth] ditampilkan sebagai contoh. Dalam gambar ini, kita tidak bisa menentukan apakah objek itu adalah orang sungguhan atau mainan kecil murni berdasarkan gambar itu sendiri. Kecuali kita mengubah sudut pandang kita, dengan menjelajahi struktur tiga dimensi dari _scene_ itu. Dengan kata lain, dari satu gambar, kita tidak dapat menentukan ukuran sebenarnya dari sebuah objek. Ini mungkin objek yang besar tapi juga jauh, atau mungkin juga objek yang dekat tapi kecil. Ukurannya mungkin tampak sama dengan gambar karena efek proyeksi perspektif.

![why-depth](/resources/whatIsSLAM/why-depth.jpg)

Semenjak gambar yang diambil oleh kamera _monocular_ hanyalah proyeksi 2D dari ruang 3D, jika kita ingin mengembalikan struktur 3D, kita harus mengubah _angle_ kamera. SLAM _monocular_ mengadopsi prinsip yang sama. Kami memindahkan kamera dan memperkirakan **gerakannya** sendiri, serta jarak dan ukuran benda-benda di _scene_, yang dinamakan dengan **struktur** _scene_. Jadi bagaimana kita memperkirakan pergerakan dan struktur ini? Dari pengalaman kita tahu bahwa jika sebuah kamera bergerak ke kanan, benda-benda di gambar akan bergerak ke kiri - ini memberi kita inspirasi untuk menyimpulkan gerak. Di sisi lain, kita juga tahu bahwa **objek dekat akan bergerak lebih cepat, sedangkan objek jauh akan bergerak lebih lambat**. Jadi, saat kamera bergerak, pergerakan benda-benda ini akan membentuk gambar **parallax**. Dengan menghitung _parallax_, secara kuantitatif dapat kita tentukan mana objek yang jauh dan mana objek yang dekat.

Namun, jika kita tahu objek mana yang dekat dan mana yang jauh, nilainya hanya akan tetap relatif. Misalnya, ketika kita menonton film, kita bisa membedakan objek mana yang ada di film lebih besar daripada yang lain, tetapi kita tidak dapat menentukan "skala sebenarnya" dari benda-benda itu - seperti bangunan-bangunan bertingkat tinggi atau hanya model tertentu di atas meja? Apakah monster itu nyata yang menghancurkan sebuah bangunan, atau seorang aktor mengenakan pakaian khusus? Secara intuitif, jika gerakan kamera dan ukuran adegan dua kali lipat pada saat bersamaan, kamera _monocular_ juga melihat hal yang sama. Demikian juga, mengalikan ukuran ini dengan faktor apapun, kita masih akan mendapatkan gambaran yang sama. Ini menunjukkan bahwa lintasan dan peta yang diperoleh dari estimasi SLAM monocular akan berbeda dari lintasan aktual dan peta dengan faktor, yaitu yang disebut **skala** (catatan kaki: alasan matematis akan dijelaskan di bab odometer visual. ). Karena SLAM _monocular_ tidak dapat menentukan skala nyata ini semata-mata berdasarkan gambar saja, ini juga disebut **skala tidak pasti**.

Hanya dengan gerakan _translation_, kedalaman bisa dihitung, dan skala sebenarnya tidak bisa ditentukan. Dua hal ini bisa menimbulkan masalah penting saat menerapkan SLAM _monocular_. Akar penyebabnya adalah kedalaman yang tidak bisa ditentukan dari satu gambar saja. Jadi, untuk mendapatkan kedalaman yang nyata, periset mulai menggunakan kamera stereo dan kamera RGB-D.

#### Kamere Stereo dan Kamera RGB-D

Tujuan penggunaan kamera stereo dan kamera RGB-D adalah untuk mengukur jarak antara objek dan kamera, yang mana untuk mengatasi kekurangan kamera monocular yang jaraknya tidak diketahui. Begitu jarak diketahui, struktur 3D dari sebuah adegan dapat dipulihkan dari satu frame, dan dengan demikian akan menghilangkan skala tidak pasti. Meski kamera stereo dan kamera RGB-D umumnya digunakan untuk mengukur jarak, akan tetapi prinsip mereka tidak sama. Setiap kamera stereo terdiri dari dua kamera _monocular_, dipisahkan dengan jarak yang diketahui, disebut dengan **baseline**. Kami menggunakan _baseline_ ini untuk menghitung posisi 3D masing-masing pixel, dengan cara yang sangat mirip dengan mata manusia kita. Kita dapat memperkirakan jarak objek berdasarkan perbedaan antara gambar dari mata kiri dan mata kanan, dan kita akan mencoba melakukan hal yang sama di komputer (lihat gambar [stereo]). Jika kita memperpanjang kamera stereo, kita juga bisa membangun sistem multi-kamera, namun tidak ada perbedaan yang mendasar.

![stereo](/resources/whatIsSLAM/stereo.jpg)

Kamera stereo biasanya memerlukan sejumlah daya komputasi yang signifikan (tidak meyakinkan) untuk memperkirakan kedalaman setiap piksel. Ini benar-benar ceroboh dibandingkan dengan manusia. Kisaran kedalaman yang diukur oleh kamera stereo terkait dengan panjang dasar. Semakin lama garis dasar, semakin jauh ukurannya. Jadi kamera stereo yang terpasang pada kendaraan otonom biasanya cukup besar. Perkiraan kedalaman untuk kamera stereo didapat dengan membandingkan gambar dari kamera kiri dan kanan, dan tidak bergantung pada peralatan penginderaan lainnya. Dengan demikian kamera stereo bisa diaplikasikan baik secara indoor maupun outdoor. Kelemahan dari kamera stereo atau sistem multi-kamera adalah proses konfigurasi dan kalibrasinya yang rumit, serta jangkauan dan akurasi kedalaman dibatasi oleh panjang dasar dan resolusi kamera. Selain itu, perhitungan pencocokan dan perbedaan stereo juga menghabiskan banyak sumber komputasi, dan biasanya memerlukan GPU dan FPGA untuk mempercepat penghasilan peta agar kedalamannya real-time untuk gambar asli. Oleh karena itu, biaya komputasi mutakhir saat ini adalah salah satu masalah utama kamera stereo.

Kamera Kedalaman (biasa dikenal dengan kamera RGB-D, istilah RGB-D akan digunakan kedepannya dalam buku ini) adalah jenis kamera baru yang muncul sejak 2010. Serupa dengan pemindai laser, kamera RGB-D mengadopsi struktur cahaya inframerah atau prinsip Time-of-Flight (ToF), mengukur jarak antara objek dan kamera dengan memancarkan cahaya secara aktif ke objek dan menerima pantulan cahaya. Bagian ini tidak diselesaikan oleh perangkat lunak sebagai kamera stereo, namun dengan alat pengukuran fisik, sehingga bisa menghemat banyak sumber komputasi dibandingkan dengan kamera stereo (lihat gambar [rgbd]). Kamera RGB-D saat ini menggunakan Kinect / Kinect V2, Xtion Pro Live, RealSense, dll. Namun, sebagian besar kamera RGB-D masih mengalami masalah termasuk jangkauan pengukuran yang sempit, data yang ribut, bidang pandangan yang kecil, rentan terhadap gangguan sinar matahari, tidak bisa mengukur material transparan. Untuk tujuan SLAM, kamera RGB-D sebagian besar digunakan di lingkungan _indoor_, dan tidak sesuai untuk aplikasi _outdoor_.

![rgbd](/resources/whatIsSLAM/rgbd.jpg)

Kami telah membahas jenis-jenis kamera yang umum, dan kami yakin seharusnya Anda sudah mendapatkan pemahaman intuitif tentang itu. Sekarang, bayangkan sebuah kamera sedang bergerak dalam suatu _scene_, kita akan mendapatkan serangkaian gambar yang terus berubah (catatan kaki: Anda dapat mencoba menggunakan telepon Anda untuk merekam klip video.). Tujuan visual SLAM adalah melokalkan dan membangun peta dengan menggunakan gambar-gambar ini. Ini tidak sesederhana yang kita pikirkan. Ini bukanlah algoritma sederhana yang posisi _output_-nya berjalan terus dan memetakan informasi selama kita memberikan _feed_ dengan input data. SLAM membutuhkan kerangka algoritma yang baik, dan atas kerja keras berpuluh-puluh tahun oleh para peneliti, kerangka kerja tersebut telah selesai.

### Kerangka Visual Klasik SLAM

Mari kita lihat kerangka visual klasik SLAM, yang ditunjukkan pada gambar [alur kerja] berikut:

![alur kerja](/resources/whatIsSLAM/workflow.jpg)

Seluruh alur kerja visual SLAM mencakup pada langkah-langkah berikut ini:

1.  Akuisisi sensor data. Dalam visual SLAM, bagian utama ini mengacu pada akuisisi dan masa sebelum mengolah untuk gambar kamera. Untuk robot mobile, ini juga termasuk akuisisi dan sinkronisasi untuk _encoder_ motor, sensor IMU, dll.
2.  **Visual Odometer** (VO). Tugas VO adalah untuk memperkirakan pergerakan kamera antara _frame_ yang berdekatan, dan juga untuk menghasilkan peta lokal. VO juga dikenal sebagai **Front End**.
3. **Backend optimization**. _Backend_ menerima pose kamera pada perangko waktu yang berbeda dari VO, serta hasil dari deteksi loop, dan menerapkan pengoptimalan untuk menghasilkan lintasan dan peta yang konsisten secara global. Karena setelah VO terhubung, itu juga dikenal sebagai **Back End**.
4. **Loop Closing**. _Loop closing_ menentukan apakah robot telah kembali ke posisi sebelumnya. Jika sebuah loop terdeteksi, maka akan memberikan informasi ke _back end_ untuk optimasi lebih lanjut.
5. **Mapping**. Ini membangun peta tugas khusus berdasarkan perkiraan lintasan kamera.

Kerangka klasik visual SLAM adalah hasil dari usaha peneliti selama lebih dari satu dekade. Kerangka itu sendiri dan algoritma yang dikandungnya pada dasarnya telah selesai dan telah disediakan di beberapa perpustakaan visi dan robotika. Dengan mengandalkan algoritma ini, kita dapat membangun sistem visual SLAM yang melakukan pemetaan dan penempatan _real-time_ di lingkungan normal. Oleh karena itu, kita dapat menarik kesimpulan: **jika lingkungan kerja terbatas pada statis dan kaku, dengan perubahan pencahayaan yang tidak signifikan dan tidak ada campur tangan manusia**, teknologi visual SLAM sudah matang [Cadena2016].

Pembaca mungkin belum memahami konsep modul, jadi kami akan merinci fungsionalitas masing-masing modul sebagai berikut. Namun, pemahaman yang lebih dalam mengenai prinsip kerja mereka memerlukan pengetahuan matematika tertentu, dan akan diperluas di bagian kedua buku ini. Sejauh ini, pemahaman intuitif dan kualitatif pada setiap modul cukup bagus.

#### Visual Odometer

Visual odometer berkaitan dengan pergerakan kamera antara **bingkai gambar yang berdekatan**, dan kasus yang paling sederhana adalah tentu saja hubungan gerak antara dua gambar berturut-turut. Misalnya, ketika kita melihat gambar [pergerakan kamera], secara alami kita dapat mencermati bahwa gambar yang tepat seharusnya hasil gambar kiri setelah rotasi ke kiri dengan _angle_ tertentu (Anda akan paham jika ditampilkan dalam bentuk video). Mari pertimbangkan pertanyaan ini: bagaimana kita tahu pergerakan itu "belok ke kiri"? Manusia terbiasa menggunakan mata kita untuk menjelajahi dunia, dan memperkirakan posisi kita sendiri, namun intuisi ini seringkali sulit untuk dijelaskan, terutama dalam bahasa rasional. Ketika kita melihat [pergerakan kamera], tentu kita akan berpikir bahwa bar itu dekat dengan kita, sementara dinding dan papan tulis lebih jauh lagi. Saat kamera belok ke kiri, bagian yang lebih dekat dari bar mulai muncul, dan kabinet di sisi kanan mulai bergerak keluar dari pandangan kita. Dengan informasi ini, kami menyimpulkan bahwa kamera harus diputar ke kiri.

![pergerakan kamera](/resources/whatIsSLAM/cameramotion.jpg)

Namun jika kita telusuri: dapatkah kita menentukan berapa banyak kamera yang diputar dan diterjemahkan, dalam satuan derajat atau sentimeter? Masih sulit bagi kita untuk memberikan jawaban kuantitatif. Karena intuisi kita tidak bagus dalam menghitung angka. Akan tetapi bagi komputer, gerakan harus dijelaskan dalam bentuk angka. Jadi kita akan bertanya: **bagaimana seharusnya sebuah komputer menentukan gerak kamera berdasarkan gambar?**

Seperti yang sudah disebutkan sebelumnya, di bidang penglihatan komputer, tugas yang nampak wajar bagi manusia bisa sangat bertentangan dengan komputer. Gambar hanyalah matriks numerik di komputer. Sebuah komputer tidak tahu arti matrik ini (inilah masalah yang harus dipecahkan dan dipelajari oleh mesin). Pada visual SLAM, kita hanya bisa melihat blok piksel, mengetahui bahwa itu adalah hasil proyeksi dengan titik spasial ke bidang pencitraan kamera. Untuk mengukur pergerakan kamera, pertama-tama kita harus **mengerti hubungan geometrik antara kamera dan titik spasial**.

Beberapa pengetahuan dasar diperlukan untuk mengklarifikasi hubungan geometrik dan realisasi metode VO ini. Disini kita hanya ingin menyampaikan konsep intuitif. Untuk saat ini, Anda hanya perlu mengambil VO yang dapat memperkirakan gerakan kamera dari gambar bingkai yang berdekatan dan mengembalikan struktur 3D dari pemandangan. Ini dinamakan sebagai "odometer", karena mirip dengan _odometer_ sebenarnya, hanya menghitung gerakan pada saat-saat yang berdeaktan, dan tidak mempertimbangkan informasi di masa lalu. Dalam hal ini, VO seperti spesies yang hanya memiliki ingatan singkat.

Sekarang, dengan asumsi bahwa kita memiliki odometer visual, kita dapat memperkirakan pergerakan kamera di antara setiap dua frame berturut-turut. Jika kita menghubungkan gerakan yang berdekatan, ini merupakan gerakan dari lintasan robot, maka dari itu perlu membahas masalah penentuan posisi. Di sisi lain, kita bisa menghitung posisi 3D untuk setiap pixel sesuai posisi kamera pada setiap langkah waktu, dan mereka akan membentuk sebuah peta. Sampai disini, sepertinya masalah SLAM dengan VO sudah terpecahkan. Bukankah demikian?

Visual odometer memang merupakan kunci untuk memecahkan visual SLAM, kita akan menghabiskan bagian ini untuk menjelaskannya secara rinci. Namun, hanya dengan menggunakan VO untuk memperkirakan lintasan pasti akan menyebabkan **drift akumulatif**. Hal ini disebabkan oleh fakta bahwa odometer visual (dalam kasus yang paling sederhana) hanya memperkirakan pergerakan antara dua frame. Kita tahu bahwa setiap perkiraan disertai dengan kesalahan tertentu, dan karena cara kerja odometer, kesalahan dari momen sebelumnya akan dibawa ke depan sampai pada berikutnya, sehingga estimasinya tidak akurat setelah jangka waktu tertentu (lihat gambar [loopclosure]). Sebagai contoh, robot pertama-tama belok ke kiri ![](http://latex.codecogs.com/gif.latex?90^\\circ) kemudian belok ke kanan ![](http://latex.codecogs.com/gif.latex?90^\\circ). Karena ada kesalahan, kami memperkirakan ![](http://latex.codecogs.com/gif.latex?90^\\circ) pertama sebagai ![](http://latex.codecogs.com/gif.latex?89^\\circ). Kemudian kita baru menyadari bahwa setelah belok kanan, perkiraan posisi robot tidak akan kembali ke asalnya. Yang lebih buruk lagi, bahkan perkiraan berikut ini lebih akurat, mereka akan selalu membawa kesalahan ![](http://latex.codecogs.com/gif.latex?1^\\circ) dicocokkan dengan fakta sebenarnya.

![pergerakan kamera](/resources/whatIsSLAM/loopclosure.jpg)

Inilah yang disebut dengan **drift**. Ini akan menyebabkan kita tidak dapat membangun peta yang konsisten. Anda akan menemukan garis lurus asli yang miring, dan ![](http://latex.codecogs.com/gif.latex?90^\\circ) yang asli sudutnya bengkok - ini benar-benar benda yang sangat kuat! Untuk mengatasi masalah _drifting_, kita juga membutuhkan dua komponen lainnya: **optimasi back-end** (catatan kaki: biasanya dikenal dengan _back end_. Karena seringkali metode optimasi yang digunakan pada bagian ini, juga disebut dengan optimasi _back-end_.) dan **loop closing**. Loop closing bertanggung jawab untuk mendeteksi kapan robot kembali ke posisi sebelumnya, sedangkan optimasi _back-end_ mengoreksi bentuk keseluruhan lintasan berdasarkan informasi ini.

#### Optimasi Back-end

Secara umum, terutama optimasi _back-end_ mengacu pada proses penanganan **noise** pada sistem SLAM. Kami berharap agar semua data sensor akurat, namun pada kenyataannya, sensor kelas tinggi masih memiliki sejumlah keributan. Sensor kelas rendah biasanya memiliki kesalahan pengukuran yang lebih besar, sedangkan yang tinggi mungkin saja rendah. Selain itu, banyak kinerja sensor dipengaruhi oleh perubahan medan magnet, suhu, dan sebagainya. Oleh karena itu, selain untuk memecahkan masalah memperkirakan pergerakan kamera dari gambar, kami juga memperhatikan seberapa banyak perkiraan yang dikemukakan ini, bagaimana keributan ini dibawa ke depan. dari langkah terakhir ke langkah berikutnya, dan seberapa besar keyakinan yang kita miliki terhadap perkiraan saat ini. Jadi masalah yang dapat diatasi dengan optimasi _back-end_ dapat diringkas sebagai berikut: untuk memperkirakan keadaan seluruh sistem dari data yang ribut dan seberapa tidak pasti perkiraan ini. Inilah _Maximum-a-Posteriori_ (MAP). Daerah disini mencakup baik lintasan robot maupun peta lingkungan.

In contrast, the visual odometer part is usually referred to as the "front end". In a SLAM framework, the front end provides data to be optimized by the back end, as well as initial values ​​for non-linear optimization. While the back end is responsible for the overall optimization, it often only care about the data itself, instead of what sensor the data is generated from. **In visual SLAM, the front end is more relevant to computer vision research, such as image feature extraction and matching, while the back end contains mainly filtering and nonlinear optimization algorithms.**

Historically, the back-end optimization part has been equivalent to "SLAM research" for a long time. The early days, SLAM problem was described as a state estimation problem, which is exactly what the back-end optimization tries to solve. In the earliest papers on SLAM, researchers at that time called it "estimation of spatial uncertainty" [Smith1980, Smith1990]. Although it sounds obscure, it does reflect the nature of the SLAM problem: **the estimation of the uncertainty of the self-movement and the surrounding environment**. In order to solve the SLAM problem, we need state estimation theory to express the uncertainty of localization and map construction, and then use filters or nonlinear optimization to estimate the mean and uncertainty (co-variance) of the states. The details of state estimation and non-linear optimization will be described in Chapter 6, 10 and 11. Let's skip it for now.

#### Loop Closing

Loop Closing, also known as Loop Closure Detection, is mainly to address the drifting problem of position estimation in SLAM. How to solve it? Assuming that a robot has returned to its origin after a period of movement, but the estimated position does not return to the origin due to drift. How to correct it? Imagine that if there is some way to let the robot know that it has returned to the origin, or the "origin" can be identified again, we can then "pull" the estimated locations to the origin to eliminate drifts. This process is called loop closing.

Loop closing has close relationship with both "positioning" and "map building". In fact, the main purpose of building a map is to enable a robot to know places it has been to. In order to achieve loop closing, we need to let the robot has the ability to identify the scenes it has visited before. There are different alternatives to achieve this goal. For example, as we mentioned earlier, we can set a marker at where the robot starts, such as a QR code. If it sees the sign again, we know that it has returned to the origin. However, the marker is essentially an intrusive sensor, it sets constraints to the application environment (what if putting up QR code is not allowed?). We will prefer the robot can use its non-intrusive sensors, e.g. the image itself, to complete this task. One possible approach would be to detect similarities between images. This is inspired by us humans. When we see two similar images, it is easy to identify that they are taken from the same place. If the loop closing is successful, accumulative error can be significantly reduced. Therefore, visual loop detection is essentially an algorithm for calculating similarities of images. The rich information contained in images can remarkably reduce the difficulty of making correct loop detection.

After a loop is detected, we will tell the back-end optimization algorithm that "A and B are the same point". Then, based on this new information, the trajectory and the map will be adjusted to match the loop detection result. In this way, if we have sufficient and reliable loop detection, we can eliminate cumulative errors, and get globally consistent trajectories and maps.

#### Mapping

Mapping refers to the process of building a map. A map (see figure [map]) is a description of the environment, but the way of describing is not fixed and depends on the actual application.

![map](/resources/whatIsSLAM/map.jpg)

For home sweeping robots, since they mainly move on ground, a two-dimensional map with marks for open areas and obstacles would be sufficient for navigation to a certain extend. And for a camera, we need at least a three-dimensional map for its 6 degrees of freedom movement. Sometimes, we want a beautiful reconstruction result, not just a set of points, but also with texture of triangular facets. At other times, we do not care about the map, just need to know things like "point A and point B are connect, while point B and point C are not". Sometimes maps may not even be needed, or they are available from others, for example, driving vehicles can often obtain local maps plotted by others.

For maps, we have so many ideas and demands. So compared to the previously mentioned visual odometer, loop closure detection and back-end optimization, map building does not refer to a fixed form or algorithm. A collection of spatial points can be called a map, a beautiful 3D model is also a map, so is a picture of a city, a village, railways, and rivers. The form of the map depends on the application of SLAM. In general, they can be divided into to categories: **metric map** and **topological map**.

##### Metric Map

Metric maps emphasize the exact locations of objects in maps, are usually classified as either sparse or dense. Sparse metric maps abstract a scene into certain form, and do not express all the objects. For example, we can construct a sparse map by selecting representative landmarks such as road signs, and ignore other parts. In contrast, dense metric maps focus on modeling all the things that are seen. For positioning, sparse map is enough, while for navigation, a dense map is usually needed (otherwise we may hit a wall between two road signs). A dense map usually consists of a number of small pieces at a certain resolution. It can be small grids for 2D metric maps, or small voxels for 3D. Usually, a small piece may have three states to express whether an object is there: occupied, idle, and unknown. When a spatial location is queried, the map can give information about whether the location can be passed through. This type of maps can be used for a variety of navigation algorithms, such as A\*, D\* (footnote: [https://en.wikipedia.org/wiki/A*_search_algorithm](https://en.wikipedia.org/wiki/A*_search_algorithm)), etc., and thus attracts the attention of researchers in robotics field. But we can also see that this map needs to store the state of each grid, and thus being storage expensive. Even worse, in most cases, a large portion of the map details is useless. On the other hand, there is sometimes a consistency issue with large-scale metric maps. A little bit of steering error may cause the walls of two rooms to overlap and thus making the map ineffective.

##### Topological Map

Compared to the accurate metric maps, topological maps emphasize the relationships among map elements. A topological map is a graph composed of nodes and edges, only considering the connectivity between nodes. For instance, we only care about that point A and point B are connected, regardless how we could travel from point A to point B. It relaxes the requirements on precise locations of a map by removing map details, and is therefore a more compact expression. However, topological maps are not good at representing maps with complex structures. Questions such as how to split a map to form nodes and edges, and how to use a topological map for navigation and path planning, are still open problems to be studied.

### Mathematical Formulation of SLAM Problems

Through the previous introduction, readers should have gained an intuitive understanding of the modular composition of a SLAM system and the main functionality of each module. However, we cannot write runable programs only based on intuitive impressions. We want to rise it to a rational and rigorous level, that is, using mathematical language to formulate a SLAM process. We will be using variables and formulas, but please rest assured that we will try our best to keep it clear enough.

Assuming that our Little Carrot is moving in an unknown environment, carrying some sensors. How can this be described in mathematical language? First, since sensors usually collect data at different some points, we are only concerned with the locations and map at these moments. This turns a continuous period of time into discrete time steps ![](http://latex.codecogs.com/gif.latex?t=1,\\cdots,K), at which data sampling happens. We use ![](http://latex.codecogs.com/gif.latex?\\bm{x}) to indicate positions of Little Carrot. So the positions at different time steps can be written as ![](http://latex.codecogs.com/gif.latex?\\bm{x}_1,\\cdots,\\bm{x}_K), which constitute the trajectory of Little Carrot. In terms of the map, we assume that the map is made up of a number of **landmark**, and at each time step, the sensors can see a portion of the landmarks and record their observations. Assume there are in total ![](http://latex.codecogs.com/gif.latex?N) landmarks, and we will use ![](http://latex.codecogs.com/gif.latex?\\bm{y}_1,\\cdots,\\bm{y}_N) to represent them.

With such a setting, "Little Carrot move in the environment with sensors" have two aspects to be described:

1.  What is its **motion**? We want to describe how ![](http://latex.codecogs.com/gif.latex?\\bm{x}) is changed from time step ![](http://latex.codecogs.com/gif.latex?k-1) to ![](http://latex.codecogs.com/gif.latex?k).
2.  What are the **observations**? Assuming that the Little Carrot detects a certain landmark ![](http://latex.codecogs.com/gif.latex?\\bm{y}_j) at position ![](http://latex.codecogs.com/gif.latex?\\bm{x}_k) with a time stamp ![](http://latex.codecogs.com/gif.latex?k), we need to describe this event in mathematical language.

Let's first take a look at motion. Typically, a robot always carries certain kind of sensors to measure its own movement, such as an encoder or inertial sensor. These sensors can measure readings regarding its motion, but not necessarily directly the location difference. Instead, readings could be acceleration, angular velocity and other information. However, no matter what the sensor is, we can use a common and abstract mathematical model to describe it:

![](http://latex.codecogs.com/gif.latex?\\bm{x}_k=f\\left({{\\bm{x}_{k-1}},{\\bm{u}_k},\\bm{w}_k}\\right))

Where ![](http://latex.codecogs.com/gif.latex?\\bm{u}_k) is the motion sensor reading (sometimes called the **input**), while ![](http://latex.codecogs.com/gif.latex?\\bm{w}_k) is noise. Note that we use a general function ![](http://latex.codecogs.com/gif.latex?f) to describe the process, instead of specifying the way ![](http://latex.codecogs.com/gif.latex?f) works. This allows the function to represent any motion sensor, rather than being limited to a particular one, and thus becoming a general equation, . We call it the **motion equation**.

Corresponding to the motion equation, there is also a **observation equation**. The observation equation describes that when the Little Carrot sees a landmark point ![](http://latex.codecogs.com/gif.latex?\\bm{y}_j) at ![](http://latex.codecogs.com/gif.latex?\\bm{x}_k) and generates an observation data ![](http://latex.codecogs.com/gif.latex?\\bm{z}_{k,j}). Likewise, we will describe this relationship with an abstract function ![](http://latex.codecogs.com/gif.latex?h):

![](http://latex.codecogs.com/gif.latex?\\bm{z}_{k,j}=h\\left({{\\bm{y}_j},{\\bm{x}_k},\\bm{v}_{k,j}}\\right))

Where, ![](http://latex.codecogs.com/gif.latex?\\bm{v}_{k,j}) is the noise in this observation. Since there are more forms of observation sensors, the observed data ![](http://latex.codecogs.com/gif.latex?\\bm{z}) and the observed equation ![](http://latex.codecogs.com/gif.latex?h) also have many different forms.

Readers may wonder that the function ![](http://latex.codecogs.com/gif.latex?f,h) we used do not seem to specify what the motion and observations exactly are. Besides, what are ![](http://latex.codecogs.com/gif.latex?\\bm{x}), ![](http://latex.codecogs.com/gif.latex?\\bm{y}), ![](http://latex.codecogs.com/gif.latex?\\bm{z}) here? In fact, according to the actual movement of Little Carrot and the type of sensor it carries, there are different ways for **parameterization**. What is parameterization then? For example, suppose the Little Carrot moves in a plane, then its pose (footnote: In this book, we use the word "pose" to refer to "position" plus "orientation".) is described by two position values and one angle, i.e. ![](http://latex.codecogs.com/gif.latex?\\bm{x}_k=[x,y,\theta]_k^\mathrm{T}). At the same time, the motion sensor can measure the amount of change in the position and angle of Little Carrot at any time step interval ![](http://latex.codecogs.com/gif.latex?\\bm{u}_k=[\\Delta{x},\\Delta{y},\\Delta\\theta]_k^\\mathrm{T}). Then, the motion equation can be specified as 

![](http://latex.codecogs.com/gif.latex?{\\left[\\begin{array}{l}x\\\y\\\\\theta\\end{array}\\right]_k}={\\left[\\begin{array}{l}x\\\y\\\\\theta\\end{array}\\right]_{k-1}}+{\\left[\\begin{array}{l}\\Delta{x}\\\\\Delta{y}\\\\\Delta\\theta\\end{array}\\right]_k}+w_k)

This is a simple linear relationship. However, not all of the sensors can directly measure the displacement and angle changes, so there are other forms of more complex equations of motion, then we may need to carry out dynamic analysis. On the observation equation, for example, a small radish carrying a two-dimensional laser sensor. We know that when a laser sensor observes a 2D punctuation, two quantities can be measured: the distance between the road sign and the radish body is $ r $ and the angle $$ \ phi $$. The observation data is $ \ bm {y} = [p_x, p_y] ^ \ mathrm {T} $ (for the sake of simplicity, omitting the subscript), the observation data is $ \ bm {z} = [r, \ phi] ^ \ Mathrm {T} $, then the observation equation is as follows:

最后，我们在一个图片类别的evidence中加入偏置(bias)，加入偏置的目的是加入一些与输入独立无关的信息。所以图片类别的evidence为

$$ evidence\_{i}=\sum \_{j}W\_{ij}x\_{j}+b\_{i} $$

其中，\\( W\_i \\) 和 \\( b\_i \\) 分别为类别 \\( i \\) 的权值和偏置。
